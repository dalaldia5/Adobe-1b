import json
import os
from pathlib import Path
import time
from.chunker import DocumentChunker
from.search import SemanticSearcher

# --- Configuration ---
INPUT_DIR = Path("/app/input")
OUTPUT_DIR = Path("/app/output")
DOCS_DIR = INPUT_DIR / "docs"
PERSONA_PATH = INPUT_DIR / "persona.txt"
JOB_PATH = INPUT_DIR / "job.txt"

def main():
    """
    Main execution pipeline for Challenge 1B.
    """
    start_time = time.time()

    # --- 1. Validate Inputs ---
    if not all():
        print("Error: Input directories or files are missing.")
        print(f"Expected: {DOCS_DIR}, {PERSONA_PATH}, {JOB_PATH}")
        return

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # --- 2. Initialize Components ---
    chunker = DocumentChunker()
    searcher = SemanticSearcher()

    # --- 3. Process Documents: Chunking and Embedding ---
    print("Processing documents: chunking and creating embeddings...")
    all_chunks =
    pdf_files = list(DOCS_DIR.glob("*.pdf"))
    for pdf_path in pdf_files:
        try:
            chunks = chunker.process_document(pdf_path)
            all_chunks.extend(chunks)
        except Exception as e:
            print(f"Could not process {pdf_path.name}: {e}")

    if not all_chunks:
        print("No content could be chunked from the provided documents. Exiting.")
        return

    searcher.create_index(all_chunks)
    print(f"...Created {len(all_chunks)} chunks from {len(pdf_files)} documents.")

    # --- 4. Formulate Query ---
    with open(PERSONA_PATH, 'r', encoding='utf-8') as f:
        persona = f.read().strip()
    with open(JOB_PATH, 'r', encoding='utf-8') as f:
        job = f.read().strip()

    query = f"Persona: {persona}. Job to be done: {job}"
    print(f"\nExecuting search for query: \"{query}\"")

    # --- 5. Perform Search and Analysis ---
    top_sections = searcher.search(query, top_k=10)

    print("Performing sub-section analysis on top results...")
    analyzed_subsections =
    for section in top_sections:
        refined_text = searcher.summarize(section['content'], num_sentences=3)
        analyzed_subsections.append({
            "document": section['doc_name'],
            "page_number": section['page'],
            "refined_text": refined_text
        })

    # --- 6. Generate Final JSON Output ---
    output_data = {
        "metadata": {
            "input_documents": [p.name for p in pdf_files],
            "persona": persona,
            "job_to_be_done": job,
            "processing_timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        },
        "extracted_sections": [
            {
                "document": sec['doc_name'],
                "page_number": sec['page'],
                "section_title": sec.get('title', 'Paragraph'),
                "importance_rank": i + 1
            } for i, sec in enumerate(top_sections)
        ],
        "sub_section_analysis": analyzed_subsections
    }

    output_filename = OUTPUT_DIR / "challenge_1b_output.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)

    end_time = time.time()
    print(f"\nChallenge 1B complete. Output written to {output_filename}")
    print(f"Total processing time: {end_time - start_time:.2f} seconds.")

if __name__ == "__main__":
    main()