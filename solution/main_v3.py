"""
Complete persona-driven document intelligence pipeline.
Uses MiniLM + T5-small + spaCy architecture for optimal performance.
"""

import json
import time
import os
from pathlib import Path
from typing import List, Dict, Any

# Import our custom components
from chunker_v3 import TokenBasedChunker
from semantic_ranker import MiniLMSemanticRanker
from t5_summarizer_v2 import T5Summarizer

class PersonaDrivenDocumentIntelligence:
    """
    Complete ML pipeline using:
    - MiniLM-L6-v2 (~80MB, ~10ms per chunk) for semantic search
    - T5-small (~220MB, ~1s per 512-token chunk) for summarization
    - spaCy en_core_web_sm (~50MB) for NER and keywords
    - Token-based chunking (512-1024 tokens) for optimal processing
    """
    
    def __init__(self):
        print("ðŸš€ Initializing Persona-Driven Document Intelligence Pipeline")
        print("ðŸ”§ Architecture: MiniLM + T5-small + spaCy")
        
        # Initialize pipeline components
        self.chunker = TokenBasedChunker()
        self.ranker = MiniLMSemanticRanker()
        self.summarizer = T5Summarizer()
        
        # Performance tracking
        self.start_time = None
        self.stats = {
            'documents_processed': 0,
            'chunks_created': 0,
            'chunks_ranked': 0,
            'chunks_summarized': 0,
            'total_time': 0
        }
        
        print("âœ… Pipeline ready!")
    
    def load_input_files(self, input_dir: str) -> tuple:
        """Load persona and job requirements from input files."""
        print("ðŸ“‚ Loading input files...")
        
        job_file = os.path.join(input_dir, 'job.txt')
        persona_file = os.path.join(input_dir, 'persona.txt')
        
        try:
            with open(job_file, 'r', encoding='utf-8') as f:
                job = f.read().strip()
            
            with open(persona_file, 'r', encoding='utf-8') as f:
                persona = f.read().strip()
            
            print(f"ðŸ‘¤ Persona: {persona}")
            print(f"ðŸ’¼ Job: {job}")
            
            return persona, job
            
        except Exception as e:
            print(f"âŒ Error loading input files: {e}")
            raise
    
    def process_documents(self, docs_dir: str, persona: str, job: str) -> List[Dict[str, Any]]:
        """
        Complete document processing pipeline:
        1. Token-based chunking (512-1024 tokens)
        2. MiniLM semantic ranking
        3. T5-small summarization
        """
        self.start_time = time.time()
        
        print(f"ðŸ“š Processing documents from: {docs_dir}")
        print(f"ðŸŽ¯ Target: 5 PDFs in <60 seconds")
        
        # Step 1: Create chunks from all PDFs
        print("\nðŸ”§ Step 1: Token-based chunking...")
        all_chunks = []
        
        pdf_files = [f for f in os.listdir(docs_dir) if f.endswith('.pdf')]
        self.stats['documents_processed'] = len(pdf_files)
        
        print(f"ðŸ“„ Found {len(pdf_files)} PDF files")
        
        for pdf_file in pdf_files:
            pdf_path = os.path.join(docs_dir, pdf_file)
            chunks = self.chunker.process_pdf(Path(pdf_path))
            
            print(f"   {pdf_file}: {len(chunks)} chunks")
            all_chunks.extend(chunks)
        
        self.stats['chunks_created'] = len(all_chunks)
        print(f"ðŸ“Š Created {len(all_chunks)} total chunks")
        
        # Step 2: Semantic ranking with MiniLM
        print("\nðŸ” Step 2: MiniLM semantic ranking...")
        ranked_chunks = self.ranker.rank_chunks(
            all_chunks, persona, job, top_k=7
        )
        
        self.stats['chunks_ranked'] = len(ranked_chunks)
        
        # Show diversity check
        doc_distribution = {}
        for chunk in ranked_chunks[:7]:
            doc = chunk['document']
            doc_distribution[doc] = doc_distribution.get(doc, 0) + 1
        
        print(f"ðŸ“ˆ Top 7 chunks distribution:")
        for doc, count in sorted(doc_distribution.items(), key=lambda x: x[1], reverse=True):
            print(f"   {doc}: {count} chunks")
        
        # Step 3: T5-small summarization
        print("\nðŸ“ Step 3: T5-small summarization...")
        
        # Take top 7 chunks for summarization
        top_chunks = ranked_chunks[:7]
        summarized_chunks = self.summarizer.summarize_chunks(
            top_chunks, persona, job
        )
        
        self.stats['chunks_summarized'] = len(summarized_chunks)
        
        # Step 4: Create final sections
        print("\nðŸ“‹ Step 4: Creating final sections...")
        sections = summarized_chunks  # Use summarized chunks as sections
        
        # Performance summary
        total_time = time.time() - self.start_time
        self.stats['total_time'] = total_time
        
        print(f"\nâ±ï¸ Performance Summary:")
        print(f"   Total time: {total_time:.2f} seconds")
        print(f"   Documents: {self.stats['documents_processed']}")
        print(f"   Chunks created: {self.stats['chunks_created']}")
        print(f"   Chunks ranked: {self.stats['chunks_ranked']}")
        print(f"   Chunks summarized: {self.stats['chunks_summarized']}")
        print(f"   Final sections: {len(sections)}")
        print(f"   Speed: {self.stats['chunks_created']/total_time:.1f} chunks/sec")
        
        if total_time <= 60:
            print("âœ… Target achieved: <60 seconds!")
        else:
            print("âš ï¸ Performance warning: >60 seconds")
        
        return sections
    
    def format_output(self, sections: List[Dict[str, Any]], persona: str, job: str) -> Dict[str, Any]:
        """Format results to match expected output structure."""
        
        # Limit to top 7 sections for final output
        top_sections = sections[:7]
        
        # Get current timestamp
        import datetime
        timestamp = datetime.datetime.now().isoformat()
        
        # Get list of input documents
        input_documents = list(set([section.get('document', 'Unknown') for section in sections]))
        
        output = {
            "metadata": {
                "input_documents": input_documents,
                "persona": persona,
                "job_to_be_done": job,
                "processing_timestamp": timestamp,
                "processing_stats": {
                    "total_documents": self.stats['documents_processed'],
                    "total_chunks_created": self.stats['chunks_created'],
                    "total_processing_time": f"{self.stats['total_time']:.2f}s",
                    "architecture": "MiniLM + T5-small + spaCy",
                    "performance_target": "5 PDFs in <60 seconds",
                    "constraints_met": {
                        "cpu_only": True,
                        "model_size_under_1gb": True,
                        "processing_under_60s": self.stats['total_time'] <= 60,
                        "no_internet_access": True
                    }
                }
            },
            "extracted_sections": []
        }
        
        for i, section in enumerate(top_sections):
            # Main section data
            extracted_section = {
                "document": section.get('document', 'Unknown'),
                "page_number": section.get('page_number', 1),
                "section_title": section.get('section_title', f'Section {i+1}'),
                "importance_rank": i + 1,
                "sub_section_analysis": {
                    "document": section.get('document', 'Unknown'),
                    "refined_text": section.get('summary', section.get('text', '')[:500] + "..."),
                    "page_number": section.get('page_number', 1),
                    "relevance_score": round(section.get('relevance_score', 0.0), 3),
                    "keywords": section.get('keywords', []),
                    "token_count": section.get('token_count', 0)
                }
            }
            output["extracted_sections"].append(extracted_section)
        
        return output
    
    def save_output(self, output: Dict[str, Any], output_path: str):
        """Save results to JSON file."""
        print(f"ðŸ’¾ Saving results to: {output_path}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print("âœ… Results saved successfully!")


def main():
    """Main execution function."""
    
    # Initialize pipeline
    pipeline = PersonaDrivenDocumentIntelligence()
    
    # Define paths
    base_dir = os.path.dirname(os.path.abspath(__file__))
    input_dir = os.path.join(base_dir, '..', 'input')
    docs_dir = os.path.join(input_dir, 'docs')
    output_dir = os.path.join(base_dir, '..', 'output')
    output_file = os.path.join(output_dir, 'challenge_1b_output.json')
    
    try:
        # Load input requirements
        persona, job = pipeline.load_input_files(input_dir)
        
        # Process documents
        sections = pipeline.process_documents(docs_dir, persona, job)
        
        # Format and save output
        output = pipeline.format_output(sections, persona, job)
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Save results
        pipeline.save_output(output, output_file)
        
        # Summary
        print(f"\nðŸŽ‰ Pipeline Complete!")
        print(f"ðŸ“Š Processed {pipeline.stats['documents_processed']} documents")
        print(f"â±ï¸ Total time: {pipeline.stats['total_time']:.2f} seconds")
        print(f"ðŸ“„ Generated {len(sections)} relevant sections")
        
    except Exception as e:
        print(f"âŒ Pipeline error: {e}")
        raise


if __name__ == "__main__":
    main()
